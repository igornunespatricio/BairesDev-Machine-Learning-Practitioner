{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/igornunespatricio/BairesDev-Machine-Learning-Practitioner/blob/main/face_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Face Recognition\n",
        "\n",
        "Steps\n",
        "\n",
        "1) Upload raw images in directory uploaded_folder\n",
        "\n",
        "2) Detect faces using MTCNN and store in the faces directory\n",
        "\n",
        "3) Use pre-trained model InceptionResnetV1 to extract embeddings from faces in the faces directory\n",
        "\n",
        "4) Use DBSCAN to cluster the embeddings and generate the organized_faces directory automatically with a folder for each person\n",
        "\n",
        "5) Encoding known faces: generate a pickle file with known face embeddings\n",
        "\n",
        "6) Recognize faces: upload unseen images and compare detected faces with known embeddings."
      ],
      "metadata": {
        "id": "QsDi1sRJjgJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastcore -U\n",
        "!pip install Pillow==9.3.0 --force-reinstall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "Lv2etTD4kh-Y",
        "outputId": "ad978602-f548-47ac-e0c7-c7b678cd19b1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastcore in /usr/local/lib/python3.10/dist-packages (1.7.20)\n",
            "Collecting fastcore\n",
            "  Downloading fastcore-1.7.22-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fastcore) (24.2)\n",
            "Downloading fastcore-1.7.22-py3-none-any.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.7/83.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fastcore\n",
            "  Attempting uninstall: fastcore\n",
            "    Found existing installation: fastcore 1.7.20\n",
            "    Uninstalling fastcore-1.7.20:\n",
            "      Successfully uninstalled fastcore-1.7.20\n",
            "Successfully installed fastcore-1.7.22\n",
            "Collecting Pillow==9.3.0\n",
            "  Downloading Pillow-9.3.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Downloading Pillow-9.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Pillow\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: pillow 11.0.0\n",
            "    Uninstalling pillow-11.0.0:\n",
            "      Successfully uninstalled pillow-11.0.0\n",
            "Successfully installed Pillow-9.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              },
              "id": "59f5de102a384524aa6af7405682a39e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gpZTS4INpa7Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c97bd67-d769-42e6-8c8f-86d90941f778"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for keras-facenet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.1/100.1 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow opencv-python mtcnn keras-facenet facenet-pytorch --quiet\n",
        "!pip install face_recognition numpy --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Upload raw images from local\n",
        "\n",
        "Ask user to upload raw images from local to colab and store it in the uploaded_images directory."
      ],
      "metadata": {
        "id": "WWYiWVqJlKYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "upload_dir = 'uploaded_images'\n",
        "if not os.path.exists(upload_dir):\n",
        "    os.makedirs(upload_dir)\n",
        "\n",
        "# Upload images\n",
        "uploaded = files.upload()\n",
        "\n",
        "print(\"\\nFiles uploaded successfully.\")\n",
        "\n",
        "# Move uploaded images to the designated directory\n",
        "for filename in uploaded.keys():\n",
        "    os.rename(filename, os.path.join(upload_dir, filename))\n",
        "\n",
        "print(f\"\\nUploaded files saved to '{upload_dir}' directory.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "y8SwGssZ33iD",
        "outputId": "c42a8c17-fd27-4293-d05b-27e63ccbbe81"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-79ae4850-f683-4683-80e4-ed86f6b8e0a1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-79ae4850-f683-4683-80e4-ed86f6b8e0a1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving WhatsApp Image 2024-11-30 at 15.35.39 (2).jpeg to WhatsApp Image 2024-11-30 at 15.35.39 (2).jpeg\n",
            "Saving WhatsApp Image 2024-11-30 at 15.35.39 (1).jpeg to WhatsApp Image 2024-11-30 at 15.35.39 (1).jpeg\n",
            "Saving WhatsApp Image 2024-11-30 at 15.35.39.jpeg to WhatsApp Image 2024-11-30 at 15.35.39.jpeg\n",
            "Saving WhatsApp Image 2024-11-30 at 15.35.38 (1).jpeg to WhatsApp Image 2024-11-30 at 15.35.38 (1).jpeg\n",
            "Saving WhatsApp Image 2024-11-30 at 15.35.38.jpeg to WhatsApp Image 2024-11-30 at 15.35.38.jpeg\n",
            "Saving WhatsApp Image 2024-11-30 at 15.33.35.jpeg to WhatsApp Image 2024-11-30 at 15.33.35.jpeg\n",
            "Saving WhatsApp Image 2024-11-30 at 15.11.29 (3).jpeg to WhatsApp Image 2024-11-30 at 15.11.29 (3).jpeg\n",
            "Saving WhatsApp Image 2024-11-30 at 15.11.29 (2).jpeg to WhatsApp Image 2024-11-30 at 15.11.29 (2).jpeg\n",
            "Saving WhatsApp Image 2024-11-30 at 15.11.29 (1).jpeg to WhatsApp Image 2024-11-30 at 15.11.29 (1).jpeg\n",
            "Saving WhatsApp Image 2024-11-30 at 15.11.29.jpeg to WhatsApp Image 2024-11-30 at 15.11.29.jpeg\n",
            "Saving WhatsApp Image 2024-11-30 at 15.11.28 (3).jpeg to WhatsApp Image 2024-11-30 at 15.11.28 (3).jpeg\n",
            "Saving WhatsApp Image 2024-11-30 at 15.11.28 (2).jpeg to WhatsApp Image 2024-11-30 at 15.11.28 (2).jpeg\n",
            "Saving WhatsApp Image 2024-11-30 at 15.11.28 (1).jpeg to WhatsApp Image 2024-11-30 at 15.11.28 (1).jpeg\n",
            "Saving WhatsApp Image 2024-11-30 at 15.11.28.jpeg to WhatsApp Image 2024-11-30 at 15.11.28.jpeg\n",
            "Saving WhatsApp Image 2024-11-30 at 15.11.27 (3).jpeg to WhatsApp Image 2024-11-30 at 15.11.27 (3).jpeg\n",
            "Saving WhatsApp Image 2024-11-30 at 15.11.27 (2).jpeg to WhatsApp Image 2024-11-30 at 15.11.27 (2).jpeg\n",
            "Saving WhatsApp Image 2024-11-30 at 15.11.27 (1).jpeg to WhatsApp Image 2024-11-30 at 15.11.27 (1).jpeg\n",
            "Saving WhatsApp Image 2024-11-30 at 15.11.27.jpeg to WhatsApp Image 2024-11-30 at 15.11.27.jpeg\n",
            "Saving WhatsApp Image 2024-11-30 at 15.11.26.jpeg to WhatsApp Image 2024-11-30 at 15.11.26.jpeg\n",
            "Saving WhatsApp Image 2024-11-29 at 22.18.20 (1).jpeg to WhatsApp Image 2024-11-29 at 22.18.20 (1).jpeg\n",
            "Saving WhatsApp Image 2024-11-29 at 22.18.20.jpeg to WhatsApp Image 2024-11-29 at 22.18.20.jpeg\n",
            "Saving WhatsApp Image 2024-11-29 at 22.18.19 (1).jpeg to WhatsApp Image 2024-11-29 at 22.18.19 (1).jpeg\n",
            "Saving WhatsApp Image 2024-11-29 at 22.18.19.jpeg to WhatsApp Image 2024-11-29 at 22.18.19.jpeg\n",
            "Saving WhatsApp Image 2024-11-29 at 22.18.18.jpeg to WhatsApp Image 2024-11-29 at 22.18.18.jpeg\n",
            "Saving WhatsApp Image 2024-11-29 at 18.44.25 (2).jpeg to WhatsApp Image 2024-11-29 at 18.44.25 (2).jpeg\n",
            "Saving WhatsApp Image 2024-11-29 at 18.44.25 (1).jpeg to WhatsApp Image 2024-11-29 at 18.44.25 (1).jpeg\n",
            "Saving WhatsApp Image 2024-11-29 at 18.44.25.jpeg to WhatsApp Image 2024-11-29 at 18.44.25.jpeg\n",
            "Saving WhatsApp Image 2024-11-29 at 18.44.24 (2).jpeg to WhatsApp Image 2024-11-29 at 18.44.24 (2).jpeg\n",
            "Saving WhatsApp Image 2024-11-29 at 18.44.24 (1).jpeg to WhatsApp Image 2024-11-29 at 18.44.24 (1).jpeg\n",
            "Saving WhatsApp Image 2024-11-29 at 18.44.24.jpeg to WhatsApp Image 2024-11-29 at 18.44.24.jpeg\n",
            "\n",
            "Files uploaded successfully.\n",
            "\n",
            "Uploaded files saved to 'uploaded_images' directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "from mtcnn import MTCNN"
      ],
      "metadata": {
        "id": "czZtDKD08zmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) MTCNN model to detect faces\n",
        "\n",
        "Run MTCNN pre trained model to detect faces on the images of the uploaded_images directory and store the faces in the faces directory."
      ],
      "metadata": {
        "id": "aFwhTbzclOta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the MTCNN face detector\n",
        "detector = MTCNN()\n",
        "\n",
        "# Path to the uploaded images\n",
        "image_folder = '/content/uploaded_images'\n",
        "\n",
        "# Create a folder to save cropped faces\n",
        "os.makedirs('/content/faces', exist_ok=True)\n",
        "\n",
        "# Loop through the images in the folder\n",
        "for filename in os.listdir(image_folder):\n",
        "    img_path = os.path.join(image_folder, filename)\n",
        "\n",
        "    # Read the image\n",
        "    img = cv2.imread(img_path)\n",
        "\n",
        "    # Convert the image to RGB (OpenCV loads images in BGR)\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Detect faces in the image\n",
        "    faces = detector.detect_faces(img_rgb)\n",
        "\n",
        "    # Crop faces and save them\n",
        "    for i, face in enumerate(faces):\n",
        "        x, y, w, h = face['box']\n",
        "        face_img = img[y:y+h, x:x+w]\n",
        "        cropped_face_path = f'/content/faces/{filename}_face_{i}.jpg'\n",
        "        cv2.imwrite(cropped_face_path, face_img)\n"
      ],
      "metadata": {
        "id": "sDMRI4Ah3Kxt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) InceptionResnetV1 to extract embeddings\n",
        "\n",
        "Use of pre trained InceptionResnetV1 model to extract embeddings from the detected faces in the faces directory."
      ],
      "metadata": {
        "id": "JcigihnQnZc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.cluster import DBSCAN\n",
        "from facenet_pytorch import InceptionResnetV1\n",
        "import torch\n",
        "\n",
        "# Path to the folder containing the face images\n",
        "face_folder = '/content/faces'\n",
        "\n",
        "# Load the pre-trained InceptionResnetV1 model for face embedding extraction\n",
        "inception = InceptionResnetV1(pretrained='vggface2').eval()\n",
        "\n",
        "# Function to extract embeddings from the face image\n",
        "def get_face_embedding(face_image):\n",
        "    # Convert the image from BGR (OpenCV) to RGB\n",
        "    face_image_rgb = cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Resize the image if necessary (standardize size for embedding generation)\n",
        "    face_image_rgb = cv2.resize(face_image_rgb, (160, 160))  # Resize to 160x160 for the model\n",
        "\n",
        "    # Convert the image to a tensor\n",
        "    face_tensor = torch.tensor(face_image_rgb).permute(2, 0, 1).float() / 255.0  # Normalize to [0, 1] range\n",
        "    face_tensor = face_tensor.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    # Get the face embedding\n",
        "    embedding = inception(face_tensor)\n",
        "    return embedding.detach().numpy()  # Convert tensor to numpy array\n",
        "\n",
        "# List to store face images and embeddings\n",
        "face_embeddings = []\n",
        "\n",
        "# Loop through the images in the 'faces' folder and generate embeddings\n",
        "for filename in os.listdir(face_folder):\n",
        "    img_path = os.path.join(face_folder, filename)\n",
        "\n",
        "    # Read the image\n",
        "    img = cv2.imread(img_path)\n",
        "    if img is not None:\n",
        "        # Get the embedding for the detected face\n",
        "        embedding = get_face_embedding(img)\n",
        "\n",
        "        # Only store the embedding if it was successfully extracted\n",
        "        if embedding is not None:\n",
        "            face_embeddings.append(embedding)  # Store the embedding\n",
        "\n",
        "# Convert embeddings list to numpy array for clustering\n",
        "face_embeddings_np = np.array(face_embeddings)\n",
        "\n",
        "# Reshape the embeddings array to 2D\n",
        "# The embeddings are currently in shape (n_samples, 1, embedding_size)\n",
        "# We need to reshape it to (n_samples, embedding_size) for DBSCAN\n",
        "face_embeddings_np = face_embeddings_np.reshape(face_embeddings_np.shape[0], -1)"
      ],
      "metadata": {
        "id": "B5xCmrDdSAes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) DBSCAN to cluster embeddings\n",
        "\n",
        "Use DBSCAN to cluster similar faces according to their embeddings and automatically generate a folder for each person."
      ],
      "metadata": {
        "id": "pJ6EfaVvn595"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform DBSCAN clustering\n",
        "eps = 1  # Maximum distance between points in the same cluster\n",
        "min_samples = 2  # Minimum number of points to form a dense cluster\n",
        "dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "labels = dbscan.fit_predict(face_embeddings_np)\n",
        "\n",
        "\n",
        "# Create a folder to save the clustered faces\n",
        "clustered_folder = '/content/organized_faces'\n",
        "\n",
        "# Create a folder for noise points (outliers)\n",
        "noise_folder = os.path.join(clustered_folder, 'noise')\n",
        "os.makedirs(noise_folder, exist_ok=True)\n",
        "\n",
        "# Save faces into folders based on their cluster labels\n",
        "for label in set(labels):\n",
        "    if label == -1:\n",
        "        # DBSCAN assigns -1 to noise points (outliers), save these faces in the 'noise' folder\n",
        "        for i, label_i in enumerate(labels):\n",
        "            if label_i == -1:\n",
        "                # Construct the filename for the face image\n",
        "                face_filename = f'face_{i}.jpg'\n",
        "                face_path = os.path.join(noise_folder, face_filename)\n",
        "                cv2.imwrite(face_path, cv2.imread(os.path.join(face_folder, os.listdir(face_folder)[i])))  # Save the noise face\n",
        "        continue\n",
        "\n",
        "    # Create a folder for each cluster\n",
        "    cluster_folder = os.path.join(clustered_folder, f'person_{label}')\n",
        "    os.makedirs(cluster_folder, exist_ok=True)\n",
        "\n",
        "    # Loop through the images and move them to the corresponding cluster folder\n",
        "    for i, label_i in enumerate(labels):\n",
        "        if label_i == label:\n",
        "            # Construct the filename for the face image\n",
        "            face_filename = f'face_{i}.jpg'\n",
        "            face_path = os.path.join(cluster_folder, face_filename)\n",
        "            cv2.imwrite(face_path, cv2.imread(os.path.join(face_folder, os.listdir(face_folder)[i])))  # Save the face image\n",
        "\n",
        "print(\"Faces organized into clusters, and noise faces are saved separately.\")"
      ],
      "metadata": {
        "id": "U-cLHAbIn4pu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "count = Counter(labels)\n",
        "\n",
        "# Print the number of faces in each class\n",
        "for class_value, num_faces in count.items():\n",
        "    print(f\"Number of faces in class {class_value}: {num_faces}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCyj_Px_hiZT",
        "outputId": "3012faee-c117-4e1a-c6c1-9e977e485bf0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of faces in class 0: 21\n",
            "Number of faces in class 1: 16\n",
            "Number of faces in class 2: 3\n",
            "Number of faces in class -1: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5) Encoding known faces\n",
        "\n",
        "Take the faces and encode them in the embedding."
      ],
      "metadata": {
        "id": "Qx-dbavB2U5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "def encode_faces(dataset_path):\n",
        "    known_face_encodings = []\n",
        "    known_face_names = []\n",
        "\n",
        "    for person_name in os.listdir(dataset_path):\n",
        "        person_folder = os.path.join(dataset_path, person_name)\n",
        "        for image_name in os.listdir(person_folder):\n",
        "            image_path = os.path.join(person_folder, image_name)\n",
        "            img = cv2.imread(image_path)\n",
        "            if img is not None:\n",
        "                embedding = get_face_embedding(img)\n",
        "                if embedding is not None:\n",
        "                    known_face_encodings.append(embedding)\n",
        "                    known_face_names.append(person_name)\n",
        "\n",
        "    # Save encodings and names to a pickle file\n",
        "    with open(\"known_faces.pkl\", \"wb\") as f:\n",
        "        pickle.dump((known_face_encodings, known_face_names), f)\n",
        "\n",
        "# Run the encoding function\n",
        "encode_faces(\"/content/organized_faces\")\n"
      ],
      "metadata": {
        "id": "psextScG2inH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6) Recognize faces"
      ],
      "metadata": {
        "id": "a_gQ84-L2r70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "upload_dir = 'predict-these-images'\n",
        "if not os.path.exists(upload_dir):\n",
        "    os.makedirs(upload_dir)\n",
        "\n",
        "# Upload images\n",
        "uploaded = files.upload()\n",
        "\n",
        "print(\"\\nFiles uploaded successfully.\")\n",
        "\n",
        "# Move uploaded images to the designated directory\n",
        "for filename in uploaded.keys():\n",
        "    os.rename(filename, os.path.join(upload_dir, filename))\n",
        "\n",
        "print(f\"\\nUploaded files saved to '{upload_dir}' directory.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "id": "DuiiutU3YGqb",
        "outputId": "426ccccc-867c-40f0-94c9-03fc8f888467"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1a034b60-7a7b-40ea-adac-beaba4032da5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1a034b60-7a7b-40ea-adac-beaba4032da5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving WhatsApp Image 2024-12-02 at 20.44.18.jpeg to WhatsApp Image 2024-12-02 at 20.44.18.jpeg\n",
            "Saving WhatsApp Image 2024-12-02 at 20.44.19.jpeg to WhatsApp Image 2024-12-02 at 20.44.19.jpeg\n",
            "\n",
            "Files uploaded successfully.\n",
            "\n",
            "Uploaded files saved to 'predict-these-images' directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow  # If using Google Colab\n",
        "\n",
        "# Load known face embeddings and names\n",
        "with open('/content/known_faces.pkl', 'rb') as f:\n",
        "    known_face_encodings, known_face_names = pickle.load(f)\n",
        "\n",
        "known_face_encodings = np.array(known_face_encodings).squeeze(axis=1)  # Now shape is (41, 512)\n",
        "\n",
        "# Initialize face detector\n",
        "detector = MTCNN()\n",
        "\n",
        "# Path to the directory containing images to predict\n",
        "directory = '/content/predict-these-images'\n",
        "\n",
        "results_path = '/content/results'\n",
        "\n",
        "os.makedirs(results_path, exist_ok=True)\n",
        "\n",
        "# Loop through all images in the directory\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):  # Process only image files\n",
        "        image_path = os.path.join(directory, filename)\n",
        "        img = cv2.imread(image_path)\n",
        "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Detect faces in the image\n",
        "        faces = detector.detect_faces(img_rgb)\n",
        "\n",
        "        for face in faces:\n",
        "            x, y, w, h = face['box']\n",
        "            face_img = img[y:y+h, x:x+w]\n",
        "\n",
        "            # Generate embedding for the detected face\n",
        "            unknown_embedding = get_face_embedding(face_img)\n",
        "\n",
        "            if unknown_embedding is not None:\n",
        "                unknown_embedding = np.array(unknown_embedding).squeeze(axis=0)  # Now shape is (512,)\n",
        "                distances = np.linalg.norm(known_face_encodings - unknown_embedding, axis=1)\n",
        "\n",
        "                # Find the best match\n",
        "                min_distance = np.min(distances)\n",
        "                min_distance_index = np.argmin(distances)\n",
        "\n",
        "                # Set a threshold for recognition\n",
        "                threshold = 1.0  # Adjust this value as needed\n",
        "\n",
        "                if min_distance <= threshold:\n",
        "                    name = known_face_names[min_distance_index]\n",
        "                    print(f\"Recognized: {name} in {filename} (Distance: {min_distance})\")\n",
        "                    cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "                    cv2.putText(img, name, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "                else:\n",
        "                    print(f\"Unknown face in {filename}\")\n",
        "                    cv2.rectangle(img, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
        "                    cv2.putText(img, \"Unknown\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
        "\n",
        "        # Display or save the image with bounding boxes and labels\n",
        "        # cv2_imshow(img)  # If using Google Colab\n",
        "        cv2.imwrite(f'{results_path}/{filename}', img)  # Save the result with the same filename in a 'results' folder\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAyFXyGnwX9p",
        "outputId": "d1acc6d2-87fb-48c8-8157-05ce21a4b4de"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recognized: person_0 in WhatsApp Image 2024-12-02 at 20.44.19.jpeg (Distance: 0.7033995389938354)\n",
            "Recognized: person_1 in WhatsApp Image 2024-12-02 at 20.44.19.jpeg (Distance: 0.7914150357246399)\n",
            "Recognized: person_0 in WhatsApp Image 2024-12-02 at 20.44.18.jpeg (Distance: 0.4772179424762726)\n",
            "Recognized: person_1 in WhatsApp Image 2024-12-02 at 20.44.18.jpeg (Distance: 0.6739935278892517)\n",
            "Recognized: person_2 in WhatsApp Image 2024-12-02 at 20.44.18.jpeg (Distance: 0.4197559952735901)\n"
          ]
        }
      ]
    }
  ]
}